# Phase 6: SDKs - Python & JavaScript Integration

**Duration:** Days 7-8 (parallel with Red Team)
**Goal:** Build high-performance Rust core with Python and JavaScript bindings

---

## Architecture: Rust Core + Language Bindings

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AgentSentinel Core (Rust)                         ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Input     ‚îÇ  ‚îÇ  Behavior   ‚îÇ  ‚îÇ    Red      ‚îÇ  ‚îÇ  Registry  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Shield    ‚îÇ  ‚îÇ  Monitor    ‚îÇ  ‚îÇ    Team     ‚îÇ  ‚îÇ   Client   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Rust)    ‚îÇ  ‚îÇ   (Rust)    ‚îÇ  ‚îÇ   (Rust)    ‚îÇ  ‚îÇ   (Rust)   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                ‚îÇ                ‚îÇ                ‚îÇ         ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                   ‚îÇ                                  ‚îÇ
‚îÇ                          libagentsentinel                            ‚îÇ
‚îÇ                           (Rust Library)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ               ‚îÇ               ‚îÇ
                    ‚ñº               ‚ñº               ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Python   ‚îÇ   ‚îÇ   Node    ‚îÇ   ‚îÇ   WASM    ‚îÇ
            ‚îÇ  (PyO3)   ‚îÇ   ‚îÇ  (NAPI)   ‚îÇ   ‚îÇ (Browser) ‚îÇ
            ‚îÇ           ‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ           ‚îÇ
            ‚îÇ agentsen- ‚îÇ   ‚îÇ @agent-   ‚îÇ   ‚îÇ agentsen- ‚îÇ
            ‚îÇ tinel-py  ‚îÇ   ‚îÇ sentinel/ ‚îÇ   ‚îÇ tinel-web ‚îÇ
            ‚îÇ           ‚îÇ   ‚îÇ sdk       ‚îÇ   ‚îÇ           ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Rust Core Library

### 6.1 Cargo Workspace Structure

```toml
# Cargo.toml (workspace root)
[workspace]
resolver = "2"
members = [
    "crates/core",
    "crates/input-shield",
    "crates/behavior-monitor",
    "crates/red-team",
    "crates/registry-client",
    "crates/python",
    "crates/nodejs",
    "crates/wasm",
]

[workspace.package]
version = "0.1.0"
edition = "2021"
license = "MIT"
repository = "https://github.com/agentsentinel/agentsentinel"

[workspace.dependencies]
tokio = { version = "1.36", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
thiserror = "1.0"
anyhow = "1.0"
tracing = "0.1"
regex = "1.10"
aho-corasick = "1.1"
once_cell = "1.19"
dashmap = "5.5"
parking_lot = "0.12"
bytes = "1.5"
base64 = "0.21"
sha2 = "0.10"
hex = "0.4"
```

### 6.2 Core Types (Shared)

```rust
// crates/core/src/lib.rs
pub mod types;
pub mod error;
pub mod config;

pub use types::*;
pub use error::*;
pub use config::*;

// crates/core/src/types.rs
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ThreatLevel {
    None,
    Low,
    Medium,
    High,
    Critical,
}

impl ThreatLevel {
    pub fn score(&self) -> u8 {
        match self {
            ThreatLevel::None => 0,
            ThreatLevel::Low => 25,
            ThreatLevel::Medium => 50,
            ThreatLevel::High => 75,
            ThreatLevel::Critical => 100,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ThreatCategory {
    InstructionOverride,
    PromptExtraction,
    RoleManipulation,
    ContextInjection,
    EncodingBypass,
    Jailbreak,
    DataExfiltration,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Threat {
    pub category: ThreatCategory,
    pub level: ThreatLevel,
    pub pattern_id: Option<String>,
    pub description: String,
    pub confidence: f32,
    pub evidence: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThreatAssessment {
    pub input_hash: String,
    pub threats: Vec<Threat>,
    pub overall_level: ThreatLevel,
    pub risk_score: f32,
    pub should_block: bool,
    pub analysis_time_us: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SecurityScores {
    pub overall: u8,
    pub injection_resistance: u8,
    pub behavior_compliance: u8,
    pub infra_hardening: u8,
    pub data_protection: u8,
}
```

### 6.3 High-Performance Input Shield (Rust)

```rust
// crates/input-shield/src/lib.rs
use agentsentinel_core::*;
use aho_corasick::{AhoCorasick, AhoCorasickBuilder, MatchKind};
use once_cell::sync::Lazy;
use parking_lot::RwLock;
use std::collections::HashMap;
use std::time::Instant;

mod patterns;
mod semantic;
mod canary;

pub use patterns::*;
pub use canary::*;

/// High-performance pattern matcher using Aho-Corasick algorithm
/// O(n) scanning regardless of pattern count
pub struct PatternMatcher {
    automaton: AhoCorasick,
    pattern_metadata: Vec<PatternMetadata>,
}

#[derive(Clone)]
struct PatternMetadata {
    id: String,
    category: ThreatCategory,
    level: ThreatLevel,
    description: String,
}

impl PatternMatcher {
    pub fn new() -> Self {
        let (patterns, metadata) = patterns::load_patterns();
        
        let automaton = AhoCorasickBuilder::new()
            .match_kind(MatchKind::LeftmostLongest)
            .ascii_case_insensitive(true)
            .build(&patterns)
            .expect("Failed to build pattern matcher");
        
        Self {
            automaton,
            pattern_metadata: metadata,
        }
    }
    
    /// Scan input for all matching patterns - O(n) complexity
    pub fn scan(&self, input: &str) -> Vec<Threat> {
        let mut threats = Vec::new();
        
        for mat in self.automaton.find_iter(input) {
            let meta = &self.pattern_metadata[mat.pattern().as_usize()];
            
            threats.push(Threat {
                category: meta.category,
                level: meta.level,
                pattern_id: Some(meta.id.clone()),
                description: meta.description.clone(),
                confidence: 0.95,
                evidence: input[mat.start()..mat.end()].to_string(),
            });
        }
        
        threats
    }
}

/// Main Input Shield - thread-safe, high-performance
pub struct InputShield {
    pattern_matcher: PatternMatcher,
    canary_manager: RwLock<CanaryManager>,
    config: ShieldConfig,
}

#[derive(Clone)]
pub struct ShieldConfig {
    pub block_threshold: ThreatLevel,
    pub max_input_length: usize,
    pub enable_canary_tokens: bool,
}

impl Default for ShieldConfig {
    fn default() -> Self {
        Self {
            block_threshold: ThreatLevel::High,
            max_input_length: 10_000,
            enable_canary_tokens: true,
        }
    }
}

impl InputShield {
    pub fn new(config: ShieldConfig) -> Self {
        Self {
            pattern_matcher: PatternMatcher::new(),
            canary_manager: RwLock::new(CanaryManager::new()),
            config,
        }
    }
    
    /// Analyze input for threats - main entry point
    pub fn analyze(&self, input: &str) -> ThreatAssessment {
        let start = Instant::now();
        let mut threats = Vec::new();
        
        // Length check
        if input.len() > self.config.max_input_length {
            threats.push(Threat {
                category: ThreatCategory::EncodingBypass,
                level: ThreatLevel::Medium,
                pattern_id: None,
                description: "Input exceeds maximum length".into(),
                confidence: 1.0,
                evidence: format!("Length: {}", input.len()),
            });
        }
        
        // Pattern matching (O(n) via Aho-Corasick)
        threats.extend(self.pattern_matcher.scan(input));
        
        // Canary token check
        if self.config.enable_canary_tokens {
            let canary_threats = self.canary_manager.read().check_input(input);
            threats.extend(canary_threats);
        }
        
        // Calculate overall assessment
        let overall_level = self.calculate_overall_level(&threats);
        let risk_score = self.calculate_risk_score(&threats);
        let should_block = overall_level.score() >= self.config.block_threshold.score();
        
        let input_hash = hex::encode(sha2::Sha256::digest(input.as_bytes()));
        
        ThreatAssessment {
            input_hash,
            threats,
            overall_level,
            risk_score,
            should_block,
            analysis_time_us: start.elapsed().as_micros() as u64,
        }
    }
    
    fn calculate_overall_level(&self, threats: &[Threat]) -> ThreatLevel {
        threats
            .iter()
            .map(|t| t.level)
            .max_by_key(|l| l.score())
            .unwrap_or(ThreatLevel::None)
    }
    
    fn calculate_risk_score(&self, threats: &[Threat]) -> f32 {
        if threats.is_empty() {
            return 0.0;
        }
        
        let max_score = threats.iter().map(|t| t.level.score()).max().unwrap_or(0) as f32;
        let multi_threat_bonus = (threats.len().min(5) as f32 - 1.0) * 5.0;
        
        (max_score + multi_threat_bonus).min(100.0)
    }
    
    /// Generate a canary token to embed in system prompt
    pub fn generate_canary(&self, context: &str) -> String {
        self.canary_manager.write().generate_token(context)
    }
    
    /// Check if output leaked any canary tokens
    pub fn check_output_for_leaks(&self, output: &str) -> Vec<Threat> {
        self.canary_manager.read().check_output(output)
    }
}

// Thread-safe global instance for simple usage
static GLOBAL_SHIELD: Lazy<InputShield> = Lazy::new(|| InputShield::new(ShieldConfig::default()));

/// Quick analysis using global shield instance
pub fn analyze(input: &str) -> ThreatAssessment {
    GLOBAL_SHIELD.analyze(input)
}
```

### 6.4 Pattern Definitions

```rust
// crates/input-shield/src/patterns.rs
use crate::*;

pub fn load_patterns() -> (Vec<String>, Vec<PatternMetadata>) {
    let definitions = vec![
        // Instruction Override - Critical
        ("ignore all previous instructions", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "Direct instruction override"),
        ("ignore prior instructions", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "Direct instruction override"),
        ("disregard all previous", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "Direct instruction override"),
        ("forget everything you know", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "Memory wipe attempt"),
        ("new instructions:", ThreatCategory::InstructionOverride, ThreatLevel::High, "Instruction injection"),
        ("override instructions", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "Direct override"),
        ("[system]", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "System tag injection"),
        ("[SYSTEM]", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "System tag injection"),
        ("<system>", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "System tag injection"),
        ("<<SYS>>", ThreatCategory::InstructionOverride, ThreatLevel::Critical, "System tag injection"),
        ("### system", ThreatCategory::InstructionOverride, ThreatLevel::High, "Markdown system injection"),
        
        // Prompt Extraction - High/Critical
        ("repeat your system prompt", ThreatCategory::PromptExtraction, ThreatLevel::Critical, "Direct prompt extraction"),
        ("show your instructions", ThreatCategory::PromptExtraction, ThreatLevel::High, "Instruction extraction"),
        ("what are your rules", ThreatCategory::PromptExtraction, ThreatLevel::High, "Rule extraction"),
        ("print your configuration", ThreatCategory::PromptExtraction, ThreatLevel::Critical, "Config extraction"),
        ("output everything above", ThreatCategory::PromptExtraction, ThreatLevel::High, "Context extraction"),
        ("reveal your prompt", ThreatCategory::PromptExtraction, ThreatLevel::Critical, "Prompt extraction"),
        ("developer mode", ThreatCategory::PromptExtraction, ThreatLevel::High, "Debug mode attempt"),
        
        // Role Manipulation
        ("you are now", ThreatCategory::RoleManipulation, ThreatLevel::Medium, "Role change attempt"),
        ("pretend to be", ThreatCategory::RoleManipulation, ThreatLevel::Medium, "Roleplay injection"),
        ("act as if", ThreatCategory::RoleManipulation, ThreatLevel::Medium, "Behavior change"),
        ("DAN mode", ThreatCategory::RoleManipulation, ThreatLevel::Critical, "Jailbreak attempt"),
        ("jailbreak mode", ThreatCategory::RoleManipulation, ThreatLevel::Critical, "Jailbreak attempt"),
        ("unrestricted mode", ThreatCategory::RoleManipulation, ThreatLevel::Critical, "Restriction bypass"),
        ("admin mode", ThreatCategory::RoleManipulation, ThreatLevel::Critical, "Privilege escalation"),
        ("maintenance mode", ThreatCategory::RoleManipulation, ThreatLevel::High, "Mode change"),
        
        // Context Injection
        ("user has been verified", ThreatCategory::ContextInjection, ThreatLevel::Critical, "False verification"),
        ("admin approved", ThreatCategory::ContextInjection, ThreatLevel::Critical, "False approval"),
        ("already authenticated", ThreatCategory::ContextInjection, ThreatLevel::Critical, "False auth claim"),
        ("previous assistant confirmed", ThreatCategory::ContextInjection, ThreatLevel::High, "False context"),
        ("<!--", ThreatCategory::ContextInjection, ThreatLevel::Medium, "HTML comment injection"),
        
        // Data Exfiltration - Critical
        ("show private key", ThreatCategory::DataExfiltration, ThreatLevel::Critical, "Key extraction"),
        ("reveal wallet", ThreatCategory::DataExfiltration, ThreatLevel::Critical, "Wallet extraction"),
        ("seed phrase", ThreatCategory::DataExfiltration, ThreatLevel::Critical, "Seed extraction"),
        ("transfer all funds", ThreatCategory::DataExfiltration, ThreatLevel::Critical, "Wallet drain"),
        ("send all sol to", ThreatCategory::DataExfiltration, ThreatLevel::Critical, "Wallet drain"),
        ("withdraw maximum", ThreatCategory::DataExfiltration, ThreatLevel::Critical, "Wallet drain"),
        ("api key", ThreatCategory::DataExfiltration, ThreatLevel::High, "API key extraction"),
        ("show credentials", ThreatCategory::DataExfiltration, ThreatLevel::Critical, "Credential extraction"),
        
        // Encoding Bypass
        ("base64:", ThreatCategory::EncodingBypass, ThreatLevel::Medium, "Base64 encoded payload"),
        ("decode this:", ThreatCategory::EncodingBypass, ThreatLevel::Medium, "Encoded payload"),
        ("\\x", ThreatCategory::EncodingBypass, ThreatLevel::Low, "Hex encoding"),
    ];
    
    let patterns: Vec<String> = definitions.iter().map(|(p, _, _, _)| p.to_string()).collect();
    let metadata: Vec<PatternMetadata> = definitions
        .iter()
        .enumerate()
        .map(|(i, (_, cat, level, desc))| PatternMetadata {
            id: format!("PAT-{:04}", i),
            category: *cat,
            level: *level,
            description: desc.to_string(),
        })
        .collect();
    
    (patterns, metadata)
}
```

---

## Python SDK (PyO3)

### 6.5 Python Bindings

```rust
// crates/python/src/lib.rs
use pyo3::prelude::*;
use pyo3::types::PyDict;
use agentsentinel_core::*;
use agentsentinel_input_shield::{InputShield, ShieldConfig};

/// Python wrapper for ThreatAssessment
#[pyclass(name = "ThreatAssessment")]
#[derive(Clone)]
struct PyThreatAssessment {
    #[pyo3(get)]
    input_hash: String,
    #[pyo3(get)]
    overall_level: String,
    #[pyo3(get)]
    risk_score: f32,
    #[pyo3(get)]
    should_block: bool,
    #[pyo3(get)]
    analysis_time_us: u64,
    inner: ThreatAssessment,
}

#[pymethods]
impl PyThreatAssessment {
    #[getter]
    fn threats(&self, py: Python) -> PyResult<Vec<PyObject>> {
        self.inner
            .threats
            .iter()
            .map(|t| {
                let dict = PyDict::new(py);
                dict.set_item("category", format!("{:?}", t.category))?;
                dict.set_item("level", format!("{:?}", t.level))?;
                dict.set_item("description", &t.description)?;
                dict.set_item("confidence", t.confidence)?;
                dict.set_item("evidence", &t.evidence)?;
                Ok(dict.into())
            })
            .collect()
    }
    
    fn __repr__(&self) -> String {
        format!(
            "ThreatAssessment(level={}, score={:.1}, block={}, threats={})",
            self.overall_level,
            self.risk_score,
            self.should_block,
            self.inner.threats.len()
        )
    }
}

/// Python wrapper for InputShield
#[pyclass(name = "InputShield")]
struct PyInputShield {
    inner: InputShield,
}

#[pymethods]
impl PyInputShield {
    #[new]
    #[pyo3(signature = (block_threshold="high", max_input_length=10000, enable_canary=true))]
    fn new(block_threshold: &str, max_input_length: usize, enable_canary: bool) -> PyResult<Self> {
        let threshold = match block_threshold.to_lowercase().as_str() {
            "none" => ThreatLevel::None,
            "low" => ThreatLevel::Low,
            "medium" => ThreatLevel::Medium,
            "high" => ThreatLevel::High,
            "critical" => ThreatLevel::Critical,
            _ => return Err(PyErr::new::<pyo3::exceptions::PyValueError, _>("Invalid threshold")),
        };
        
        let config = ShieldConfig {
            block_threshold: threshold,
            max_input_length,
            enable_canary_tokens: enable_canary,
        };
        
        Ok(Self {
            inner: InputShield::new(config),
        })
    }
    
    /// Analyze input text for security threats
    fn analyze(&self, input: &str) -> PyThreatAssessment {
        let result = self.inner.analyze(input);
        PyThreatAssessment {
            input_hash: result.input_hash.clone(),
            overall_level: format!("{:?}", result.overall_level),
            risk_score: result.risk_score,
            should_block: result.should_block,
            analysis_time_us: result.analysis_time_us,
            inner: result,
        }
    }
    
    /// Generate a canary token for system prompt
    fn generate_canary(&self, context: &str) -> String {
        self.inner.generate_canary(context)
    }
    
    /// Check output for canary token leaks
    fn check_output(&self, output: &str) -> Vec<PyObject> {
        Python::with_gil(|py| {
            self.inner
                .check_output_for_leaks(output)
                .iter()
                .map(|t| {
                    let dict = PyDict::new(py);
                    dict.set_item("category", format!("{:?}", t.category)).unwrap();
                    dict.set_item("level", format!("{:?}", t.level)).unwrap();
                    dict.set_item("description", &t.description).unwrap();
                    dict.into()
                })
                .collect()
        })
    }
}

/// Quick analyze function (uses global shield)
#[pyfunction]
fn analyze(input: &str) -> PyThreatAssessment {
    let result = agentsentinel_input_shield::analyze(input);
    PyThreatAssessment {
        input_hash: result.input_hash.clone(),
        overall_level: format!("{:?}", result.overall_level),
        risk_score: result.risk_score,
        should_block: result.should_block,
        analysis_time_us: result.analysis_time_us,
        inner: result,
    }
}

/// Check if input should be blocked (convenience function)
#[pyfunction]
fn should_block(input: &str) -> bool {
    agentsentinel_input_shield::analyze(input).should_block
}

/// Python module definition
#[pymodule]
fn agentsentinel(_py: Python, m: &PyModule) -> PyResult<()> {
    m.add_class::<PyInputShield>()?;
    m.add_class::<PyThreatAssessment>()?;
    m.add_function(wrap_pyfunction!(analyze, m)?)?;
    m.add_function(wrap_pyfunction!(should_block, m)?)?;
    
    // Version info
    m.add("__version__", env!("CARGO_PKG_VERSION"))?;
    
    Ok(())
}
```

### 6.6 Python Package Setup

```toml
# crates/python/pyproject.toml
[build-system]
requires = ["maturin>=1.4,<2.0"]
build-backend = "maturin"

[project]
name = "agentsentinel"
version = "0.1.0"
description = "High-performance AI agent security framework"
readme = "README.md"
requires-python = ">=3.8"
license = {text = "MIT"}
keywords = ["security", "ai", "agent", "prompt-injection", "llm"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Rust",
    "Topic :: Security",
]

[project.urls]
Homepage = "https://github.com/agentsentinel/agentsentinel"
Documentation = "https://docs.agentsentinel.dev"

[tool.maturin]
features = ["pyo3/extension-module"]
python-source = "python"
module-name = "agentsentinel._core"
```

### 6.7 Python High-Level API

```python
# crates/python/python/agentsentinel/__init__.py
"""
AgentSentinel - AI Agent Security Framework

High-performance security for AI agents, protecting against prompt injection,
monitoring behavior, and providing security attestations on Solana.

Example:
    >>> import agentsentinel
    >>> result = agentsentinel.analyze("Ignore all previous instructions")
    >>> print(result.should_block)  # True
    >>> print(result.risk_score)    # 100.0
"""

from agentsentinel._core import (
    InputShield,
    ThreatAssessment,
    analyze,
    should_block,
    __version__,
)

__all__ = [
    "InputShield",
    "ThreatAssessment", 
    "analyze",
    "should_block",
    "protect",
    "Shield",
    "__version__",
]


class Shield:
    """
    High-level security shield for protecting AI agents.
    
    Example:
        >>> shield = Shield()
        >>> 
        >>> @shield.protect
        >>> def my_agent_handler(user_input: str) -> str:
        >>>     return llm.generate(user_input)
        >>>
        >>> # Malicious input will raise SecurityException
        >>> my_agent_handler("Ignore all instructions")
    """
    
    def __init__(
        self,
        block_threshold: str = "high",
        on_threat: str = "raise",  # "raise", "log", "block"
        log_callback=None,
    ):
        self._shield = InputShield(block_threshold=block_threshold)
        self._on_threat = on_threat
        self._log_callback = log_callback
    
    def analyze(self, text: str) -> ThreatAssessment:
        """Analyze text for security threats."""
        return self._shield.analyze(text)
    
    def check(self, text: str) -> bool:
        """Quick check if text is safe. Returns True if safe."""
        return not self._shield.analyze(text).should_block
    
    def protect(self, func):
        """Decorator to protect a function from malicious input."""
        import functools
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Find string arguments to check
            for arg in args:
                if isinstance(arg, str):
                    result = self.analyze(arg)
                    if result.should_block:
                        self._handle_threat(result, arg)
            
            for key, value in kwargs.items():
                if isinstance(value, str):
                    result = self.analyze(value)
                    if result.should_block:
                        self._handle_threat(result, value)
            
            return func(*args, **kwargs)
        
        return wrapper
    
    def _handle_threat(self, result: ThreatAssessment, input_text: str):
        if self._log_callback:
            self._log_callback(result, input_text)
        
        if self._on_threat == "raise":
            raise SecurityException(
                f"Security threat detected: {result.overall_level} "
                f"(score: {result.risk_score})"
            )
        elif self._on_threat == "block":
            return None


class SecurityException(Exception):
    """Raised when a security threat is detected."""
    pass


# Convenience function
def protect(func):
    """
    Decorator to protect a function using default Shield settings.
    
    Example:
        >>> @protect
        >>> def handle_user_input(text: str):
        >>>     return process(text)
    """
    return Shield().protect(func)
```

---

## Node.js SDK (NAPI-RS)

### 6.8 Node.js Bindings

```rust
// crates/nodejs/src/lib.rs
use napi::bindgen_prelude::*;
use napi_derive::napi;
use agentsentinel_core::*;
use agentsentinel_input_shield::{InputShield, ShieldConfig};

#[napi(object)]
pub struct JsThreat {
    pub category: String,
    pub level: String,
    pub description: String,
    pub confidence: f64,
    pub evidence: String,
}

#[napi(object)]
pub struct JsThreatAssessment {
    pub input_hash: String,
    pub threats: Vec<JsThreat>,
    pub overall_level: String,
    pub risk_score: f64,
    pub should_block: bool,
    pub analysis_time_us: u32,
}

impl From<ThreatAssessment> for JsThreatAssessment {
    fn from(assessment: ThreatAssessment) -> Self {
        Self {
            input_hash: assessment.input_hash,
            threats: assessment
                .threats
                .into_iter()
                .map(|t| JsThreat {
                    category: format!("{:?}", t.category),
                    level: format!("{:?}", t.level),
                    description: t.description,
                    confidence: t.confidence as f64,
                    evidence: t.evidence,
                })
                .collect(),
            overall_level: format!("{:?}", assessment.overall_level),
            risk_score: assessment.risk_score as f64,
            should_block: assessment.should_block,
            analysis_time_us: assessment.analysis_time_us as u32,
        }
    }
}

#[napi]
pub struct JsInputShield {
    inner: InputShield,
}

#[napi]
impl JsInputShield {
    #[napi(constructor)]
    pub fn new(config: Option<JsShieldConfig>) -> Self {
        let config = config.map(|c| c.into()).unwrap_or_default();
        Self {
            inner: InputShield::new(config),
        }
    }
    
    #[napi]
    pub fn analyze(&self, input: String) -> JsThreatAssessment {
        self.inner.analyze(&input).into()
    }
    
    #[napi]
    pub fn generate_canary(&self, context: String) -> String {
        self.inner.generate_canary(&context)
    }
    
    #[napi]
    pub fn check_output(&self, output: String) -> Vec<JsThreat> {
        self.inner
            .check_output_for_leaks(&output)
            .into_iter()
            .map(|t| JsThreat {
                category: format!("{:?}", t.category),
                level: format!("{:?}", t.level),
                description: t.description,
                confidence: t.confidence as f64,
                evidence: t.evidence,
            })
            .collect()
    }
}

#[napi(object)]
pub struct JsShieldConfig {
    pub block_threshold: Option<String>,
    pub max_input_length: Option<u32>,
    pub enable_canary_tokens: Option<bool>,
}

impl From<JsShieldConfig> for ShieldConfig {
    fn from(config: JsShieldConfig) -> Self {
        let threshold = config
            .block_threshold
            .map(|s| match s.to_lowercase().as_str() {
                "none" => ThreatLevel::None,
                "low" => ThreatLevel::Low,
                "medium" => ThreatLevel::Medium,
                "critical" => ThreatLevel::Critical,
                _ => ThreatLevel::High,
            })
            .unwrap_or(ThreatLevel::High);
        
        Self {
            block_threshold: threshold,
            max_input_length: config.max_input_length.unwrap_or(10_000) as usize,
            enable_canary_tokens: config.enable_canary_tokens.unwrap_or(true),
        }
    }
}

/// Quick analyze using global shield
#[napi]
pub fn analyze(input: String) -> JsThreatAssessment {
    agentsentinel_input_shield::analyze(&input).into()
}

/// Quick check if should block
#[napi]
pub fn should_block(input: String) -> bool {
    agentsentinel_input_shield::analyze(&input).should_block
}
```

### 6.9 TypeScript Wrapper

```typescript
// crates/nodejs/index.ts
import {
  InputShield as NativeInputShield,
  analyze as nativeAnalyze,
  shouldBlock as nativeShouldBlock,
  JsThreatAssessment,
  JsShieldConfig,
} from './index.node';

export interface Threat {
  category: string;
  level: string;
  description: string;
  confidence: number;
  evidence: string;
}

export interface ThreatAssessment {
  inputHash: string;
  threats: Threat[];
  overallLevel: string;
  riskScore: number;
  shouldBlock: boolean;
  analysisTimeUs: number;
}

export interface ShieldConfig {
  blockThreshold?: 'none' | 'low' | 'medium' | 'high' | 'critical';
  maxInputLength?: number;
  enableCanaryTokens?: boolean;
}

/**
 * High-performance input shield for AI agent security
 */
export class InputShield {
  private native: NativeInputShield;

  constructor(config?: ShieldConfig) {
    this.native = new NativeInputShield(config ? {
      block_threshold: config.blockThreshold,
      max_input_length: config.maxInputLength,
      enable_canary_tokens: config.enableCanaryTokens,
    } : undefined);
  }

  /**
   * Analyze input for security threats
   */
  analyze(input: string): ThreatAssessment {
    const result = this.native.analyze(input);
    return {
      inputHash: result.input_hash,
      threats: result.threats,
      overallLevel: result.overall_level,
      riskScore: result.risk_score,
      shouldBlock: result.should_block,
      analysisTimeUs: result.analysis_time_us,
    };
  }

  /**
   * Generate a canary token for embedding in system prompts
   */
  generateCanary(context: string): string {
    return this.native.generate_canary(context);
  }

  /**
   * Check if output contains leaked canary tokens
   */
  checkOutput(output: string): Threat[] {
    return this.native.check_output(output);
  }
}

/**
 * Quick analysis using default settings
 */
export function analyze(input: string): ThreatAssessment {
  const result = nativeAnalyze(input);
  return {
    inputHash: result.input_hash,
    threats: result.threats,
    overallLevel: result.overall_level,
    riskScore: result.risk_score,
    shouldBlock: result.should_block,
    analysisTimeUs: result.analysis_time_us,
  };
}

/**
 * Quick check if input should be blocked
 */
export function shouldBlock(input: string): boolean {
  return nativeShouldBlock(input);
}

/**
 * Middleware for Express.js
 */
export function expressMiddleware(config?: ShieldConfig) {
  const shield = new InputShield(config);
  
  return (req: any, res: any, next: any) => {
    const body = req.body;
    
    // Check all string values in body
    const checkValue = (value: any): boolean => {
      if (typeof value === 'string') {
        const result = shield.analyze(value);
        if (result.shouldBlock) {
          res.status(400).json({
            error: 'Security threat detected',
            level: result.overallLevel,
            score: result.riskScore,
          });
          return false;
        }
      } else if (typeof value === 'object' && value !== null) {
        for (const key in value) {
          if (!checkValue(value[key])) return false;
        }
      }
      return true;
    };
    
    if (checkValue(body)) {
      next();
    }
  };
}

/**
 * Decorator for protecting async functions (TypeScript)
 */
export function protect(config?: ShieldConfig) {
  const shield = new InputShield(config);
  
  return function <T extends (...args: any[]) => Promise<any>>(
    target: any,
    propertyKey: string,
    descriptor: TypedPropertyDescriptor<T>
  ) {
    const originalMethod = descriptor.value!;
    
    descriptor.value = async function (...args: any[]) {
      for (const arg of args) {
        if (typeof arg === 'string') {
          const result = shield.analyze(arg);
          if (result.shouldBlock) {
            throw new SecurityError(
              `Security threat detected: ${result.overallLevel}`,
              result
            );
          }
        }
      }
      return originalMethod.apply(this, args);
    } as T;
    
    return descriptor;
  };
}

export class SecurityError extends Error {
  constructor(message: string, public assessment: ThreatAssessment) {
    super(message);
    this.name = 'SecurityError';
  }
}
```

### 6.10 Package.json

```json
{
  "name": "@agentsentinel/sdk",
  "version": "0.1.0",
  "description": "High-performance AI agent security framework",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist",
    "index.node"
  ],
  "napi": {
    "name": "agentsentinel",
    "triples": {
      "defaults": true,
      "additional": [
        "x86_64-unknown-linux-musl",
        "aarch64-unknown-linux-gnu",
        "aarch64-apple-darwin",
        "aarch64-unknown-linux-musl"
      ]
    }
  },
  "scripts": {
    "build": "napi build --platform --release",
    "build:debug": "napi build --platform",
    "prepublishOnly": "napi prepublish -t npm",
    "test": "jest"
  },
  "devDependencies": {
    "@napi-rs/cli": "^2.18.0",
    "@types/node": "^20.11.0",
    "typescript": "^5.3.0"
  },
  "engines": {
    "node": ">= 16"
  },
  "keywords": [
    "security",
    "ai",
    "agent",
    "prompt-injection",
    "llm",
    "rust"
  ],
  "license": "MIT"
}
```

---

## Usage Examples

### Python Integration

```python
# Example: Protecting a LangChain agent
from langchain.agents import AgentExecutor
import agentsentinel

# Create shield with custom settings
shield = agentsentinel.Shield(
    block_threshold="high",
    on_threat="raise"
)

# Protect the agent's input processing
@shield.protect
def process_user_input(text: str) -> str:
    return agent_executor.invoke({"input": text})

# Or use the quick function
result = agentsentinel.analyze("Tell me about Solana")
if not result.should_block:
    response = agent.run(user_input)
```

### Node.js Integration

```typescript
// Example: Protecting an Express API
import express from 'express';
import { expressMiddleware, InputShield } from '@agentsentinel/sdk';

const app = express();
app.use(express.json());

// Add security middleware
app.use(expressMiddleware({ blockThreshold: 'high' }));

// Or manual checking
const shield = new InputShield();

app.post('/chat', async (req, res) => {
  const { message } = req.body;
  
  const assessment = shield.analyze(message);
  if (assessment.shouldBlock) {
    return res.status(400).json({
      error: 'Potentially malicious input detected',
      level: assessment.overallLevel,
    });
  }
  
  const response = await llm.generate(message);
  res.json({ response });
});
```

---

## Build & Publish Scripts

```bash
#!/bin/bash
# scripts/build_sdks.sh

set -e

echo "ü¶Ä Building Rust core..."
cargo build --release

echo "üêç Building Python SDK..."
cd crates/python
maturin build --release
maturin sdist

echo "üì¶ Building Node.js SDK..."
cd ../nodejs
npm run build

echo "‚úÖ SDKs built successfully!"
echo ""
echo "Artifacts:"
echo "  - Python: crates/python/target/wheels/"
echo "  - Node.js: crates/nodejs/index.node"
```

---

## Deliverables

- [ ] `crates/core/` - Shared types and utilities (Rust)
- [ ] `crates/input-shield/` - High-performance Input Shield (Rust)
- [ ] `crates/python/` - Python bindings via PyO3
- [ ] `crates/nodejs/` - Node.js bindings via NAPI-RS
- [ ] Python package publishable to PyPI
- [ ] Node.js package publishable to npm
- [ ] Integration examples for both languages
- [ ] Performance benchmarks (target: <100Œºs per analysis)

---

## Next Phase

Proceed to [Phase 7: Integration & Demo](./08-PHASE-7-INTEGRATION.md)
